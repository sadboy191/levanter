data:
  id: EleutherAI/pile
  tokenizer: NousResearch/Llama-2-7b-hf
model:
  type: llama
initialize_from_hf: true
use_hf_model_config: true
trainer:
  tracker:
    type: wandb
    project: "levanter"
    tags: ["pile", "llama2"]

  mp: p=f32,c=bfloat16

  model_axis_size: 1
  per_device_eval_parallelism: 4

  train_batch_size: 1024
  num_train_steps: 10000
  steps_per_eval: 500
optimizer:
  learning_rate: 1.2e-4
  weight_decay: 0.0
