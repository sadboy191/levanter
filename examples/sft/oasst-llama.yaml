model_name_or_path: NousResearch/Llama-2-7b-hf

# Training configuration
trainer:
  mp: p=f32,c=bfloat16
  wandb:
    project: "levanter-sft"
    tags: ["llama2", "oasst"]
  num_train_steps: 1218
  train_batch_size: 128

  # If using model parallelism
  tensor_parallel_axes: ["mlp", "heads"]

# Optimizer settings
optimizer:
  learning_rate: 2e-5
  weight_decay: 0.0

# Supervised data configuration
supervised_data:
  # For HF dataset
  id: "databricks/databricks-dolly-15k"
  input_field: "instruction"  # adjust based on dataset
  output_field: "response"  # adjust based on dataset
  cache_dir: "cache/dolly"

# Model configuration
max_tune_length: 2048
trust_remote_code: false
model_cache_dir: null

# Checkpoint saving configuration
hf_save_path: "sft_hf_ckpts"
hf_upload: false
hf_save_steps: 1000

# python examples/sft/sft.py --config_path examples/sft/oasst-llama2.yaml
